{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports para Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import *\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviar os dados para o HDFS\n",
    "\n",
    "os arquivos .csv devem estar dentro da pasta input do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /datasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal /input/*.csv /datasus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   2 root supergroup   62492959 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   76520681 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   91120916 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup    3046774 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /datasus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizar todos os dados do HDFS para uma tabela hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_hdfs = spark.read.csv(\"/datasus/\", header=\"true\", sep=\";\", quote=\"\\'\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_hdfs.createOrReplaceTempView(\"covid19Temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"create table covid19 as select * from covid19Temp\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table covid19Temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|             covid19|      false|\n",
      "| default|primeiravisualizacao|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processamento = \"2021-07-06\"\n",
    "data_referencia    = \"2021-07-05\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeira Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|Casos_Recuperados|Em_Acompanhamento|\n",
      "+-----------------+-----------------+\n",
      "|       17,262,646|        1,065,477|\n",
      "+-----------------+-----------------+\n",
      "\n",
      "TEMPO DE EXECUÇÃO --> 0:00:02.335001\n"
     ]
    }
   ],
   "source": [
    "# Casos Recuperados, Em acompanhamento\n",
    "\n",
    "x = datetime.now()\n",
    "\n",
    "Casos_Recuperados_DF = \\\n",
    "    sqlContext.sql( \\\n",
    "        \"SELECT \\\n",
    "            FORMAT_NUMBER(SUM(Recuperadosnovos),0) AS Casos_Recuperados, \\\n",
    "            FORMAT_NUMBER(SUM(emAcompanhamentoNovos),0) AS Em_Acompanhamento \\\n",
    "        FROM \\\n",
    "            covid19 \\\n",
    "        WHERE \\\n",
    "            data = '\" + data_processamento + \"'\").show()\n",
    "\n",
    "y = datetime.now()\n",
    "print(\"TEMPO DE EXECUÇÃO -->\", y-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos confirmados\n",
    "\n",
    "Casos_Confirmados_DF = \\\n",
    "    spark.sql( \\\n",
    "        \"SELECT \\\n",
    "            FORMAT_NUMBER((SUM(Recuperadosnovos) + SUM(emAcompanhamentoNovos) + SUM(obitosAcumulado)),0) AS Acumulado, \\\n",
    "            (\\\n",
    "                SELECT \\\n",
    "                    FORMAT_NUMBER(SUM(casosNovos),0) \\\n",
    "                FROM \\\n",
    "                    covid19 \\\n",
    "                WHERE \\\n",
    "                    data = '\" + data_referencia + \"' \\\n",
    "             ) AS Casos_Novos, \\\n",
    "            FORMAT_NUMBER(((SUM(Recuperadosnovos) + SUM(emAcompanhamentoNovos) + SUM(obitosAcumulado)) * 100000) / SUM(populacaoTCU2019), 0) as Incidencia \\\n",
    "        FROM \\\n",
    "            covid19 \\\n",
    "        WHERE \\\n",
    "            data = '\" + data_processamento + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "| Acumulado|Casos_Novos|Incidencia|\n",
      "+----------+-----------+----------+\n",
      "|19,908,799|     68,109|     3,158|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Casos_Confirmados_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terceira Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óbitos Confirmados\n",
    "\n",
    "Casos_Obitos_DF = \\\n",
    "    spark.sql( \\\n",
    "        \"SELECT \\\n",
    "            FORMAT_NUMBER(((SUM(obitosAcumulado))),0) AS Obitos_acumulados, \\\n",
    "            (\\\n",
    "                SELECT \\\n",
    "                    FORMAT_NUMBER(SUM(obitosNovos),0) \\\n",
    "                FROM \\\n",
    "                    covid19 \\\n",
    "                WHERE \\\n",
    "                    data = '\" + data_referencia + \"' \\\n",
    "            ) AS Obitos_novos, \\\n",
    "            ROUND(((SUM(obitosAcumulado)) * 100) / (SUM(Recuperadosnovos) + SUM(emAcompanhamentoNovos) + (SUM(obitosAcumulado))),2) AS Letalidade, \\\n",
    "            ROUND(((((SUM(obitosAcumulado))) * 100000)) / SUM(populacaoTCU2019), 2) AS Mortalidade \\\n",
    "        FROM \\\n",
    "            covid19 \\\n",
    "        WHERE data = '\" + data_processamento + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Casos_Obitos_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar primeira visualização em uma tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"CREATE TABLE primeiraVisualizacao as \\\n",
    "        SELECT \\\n",
    "            FORMAT_NUMBER(SUM(Recuperadosnovos),0) AS Casos_Recuperados, \\\n",
    "            FORMAT_NUMBER(SUM(emAcompanhamentoNovos),0) AS Em_Acompanhamento \\\n",
    "        FROM \\\n",
    "            covid19 \\\n",
    "        WHERE \\\n",
    "            data = '\" + data_processamento + \"'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar segunda visualização em formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Casos_Confirmados_DF.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"/user/datacovid/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2021-07-16 23:38 /user/datacovid/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        961 2021-07-16 23:38 /user/datacovid/part-00000-84a9a882-d4e0-4b35-a793-4f19211a13e8-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/datacovid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar terceira visualização como um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Casos_Obitos_DF\\\n",
    "    .selectExpr(\"'terceira' AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    "    .option(\"topic\", \"terceira-visualizacao\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = spark.read.\\\n",
    "    format(\"kafka\").\\\n",
    "    option(\"kafka.bootstrap.servers\", \"kafka:9092\").\\\n",
    "    option(\"subscribe\", \"terceira-visualizacao\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|                 key|               value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+--------------------+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|[74 65 72 63 65 6...|[7B 22 4F 62 69 7...|terceira-visualiz...|        0|     0|2021-07-17 00:26:...|            0|\n",
      "|[74 65 72 63 65 6...|[7B 22 4F 62 69 7...|terceira-visualiz...|        0|     1|2021-07-17 00:26:...|            0|\n",
      "+--------------------+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prova.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
