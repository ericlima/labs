{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports para Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "sc = spark.sparkContext\n",
    "sqlContext = HiveContext(sc)\n",
    "sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n",
    "sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "sqlContext.setConf(\"hive.exec.max.dynamic.partitions\",\"6000\")\n",
    "sqlContext.setConf(\"hive.exec.max.dynamic.partitions.pernode\",\"256\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operações HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   2 root supergroup   62492959 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   76520681 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   91120916 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup    3046774 2021-07-16 20:15 /datasus/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /datasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxrwxr-x   - root supergroup          0 2021-07-17 12:43 /user/hive/warehouse/covid19\r\n",
      "drwxrwxr-x   - root supergroup          0 2021-07-16 23:27 /user/hive/warehouse/primeiravisualizacao\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal /input/*.csv /datasus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!hdfs dfs -rm /datasus/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hive/warehouse/covid19\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /user/hive/warehouse/covid19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!hdfs dfs -rm -R /user/hive/warehouse/covid19_particionada_codmun/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use datasus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|             covid19|      false|\n",
      "| default|primeiravisualizacao|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = spark.read.csv(\"/datasus/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\", header=\"true\", sep=\";\", quote=\"\\'\", inferSchema=True)\n",
    "\n",
    "df = spark.read.csv(\"/datasus/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\", header=\"true\", sep=\";\", quote=\"\\'\", inferSchema=True)\n",
    "\n",
    "df = spark.read.csv(\"/datasus/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\", header=\"true\", sep=\";\", quote=\"\\'\", inferSchema=True)\n",
    "\n",
    "df = spark.read.csv(\"/datasus/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\", header=\"true\", sep=\";\", quote=\"\\'\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/datasus/\", header=\"true\", sep=\";\", quote=\"\\'\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"covid19_temporary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create table covid19_opt as select * from covid19_temporary\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = spark.read.table('covid19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=20, truncate=False, vertical=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.na.fill(0, [\"codmun\"]).show()\n",
    "\n",
    "df.na.fill(value=0,subset=[\"codmun\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# register as a temporary view [sql]\n",
    "df.createOrReplaceTempView(\"df_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.write.partitionBy(\"codmun\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(\"hdfs://namenode:8020/datasus/covid19_particionada_codmun2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.write.save(\"hdfs://namenode:8020/datasus/covid19_particionada_codmun3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df.write.partitionBy(\"codmun\").saveAsTable(\"covid19_particionada_codmun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql(\"SELECT codmun,count FROM df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_string = df.select(\"regiao\",\"coduf\",\"codmun\")\n",
    "topic_string.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table covid19\")\n",
    "spark.sql(\"CREATE TABLE covid19 (regiao string,estado string,municipio string,coduf int,codRegiaoSaude int,nomeRegiaoSaude string,data string,semanaEpi int,populacaoTCU2019 int,casosAcumulado int,casosNovos int,obitosAcumulado int,obitosNovos int,Recuperadosnovos int,emAcompanhamentoNovos int,interiorMetropolitana string) PARTITIONED BY (codmun int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\\n' STORED AS TEXTFILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[regiao: string, estado: string, municipio: string, coduf: int, codRegiaoSaude: int, nomeRegiaoSaude: string, data: string, semanaEpi: int, populacaoTCU2019: int, casosAcumulado: int, casosNovos: int, obitosAcumulado: int, obitosNovos: int, Recuperadosnovos: int, emAcompanhamentoNovos: int, interiorMetropolitana: string, codmun: int]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql(\"select * from covid19\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"codmun\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"covid19_particionada_codmun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) FROM covid19.parquet c19\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# primeira visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default.covid19_opt insert overwrite table datasus.covid19 partition(codmun) select regiao,estado,municipio,        coduf,        codregiaosaude,        nomeregiaosaude,        data,        semanaepi,       populacaotcu2019,        casosacumulado,        casosnovos,        obitosacumulado,        obitosnovos,        recuperadosnovos,        emacompanhamentonovos,        interiorMetropolitana,        COALESCE(codmun, 0) codmun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"from default.covid19_opt insert overwrite table datasus.covid19 partition(codmun) select regiao,estado,municipio, coduf, codregiaosaude,        nomeregiaosaude,        data,        semanaepi,       populacaotcu2019,        casosacumulado,        casosnovos,        obitosacumulado, obitosnovos, recuperadosnovos, emacompanhamentonovos, `interior/metropolitana`, COALESCE(codmun, 0) codmun  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = df.select('mycol').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT data,count(1),sum(c19.Recuperadosnovos),sum(c19.emAcompanhamentoNovos) FROM covid19 c19 where c19.regiao like 'Brasil%' group by data order by data\").show(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT c19.regiao,count(1) FROM default.covid19 c19 group by c19.regiao\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT c19.municipio,c19.estado,c19.codmun,c19.Recuperadosnovos,c19.emAcompanhamentoNovos FROM covid19 c19 where c19.codmun = 150300\").show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT c19.municipio,count(1) FROM covid19 c19 group by c19.municipio\").show(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segunda visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
